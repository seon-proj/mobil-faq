from google.cloud import aiplatform
import vertexai.preview
import vertexai
from vertexai import rag
from vertexai.generative_models import GenerativeModel, Tool
import streamlit as st
from streamlit_lottie import st_lottie

lottie_url = "https://lottie.host/832d3fa4-4c61-4362-9482-a065e8c06cd4/zY3vLCFT8N.json"

st.set_page_config(page_title="MOBILì˜ AIìƒë‹´ì‚¬ ëª¨ë¹ŒëŸ¬ ì…ë‹ˆë‹¤", page_icon="â˜ï¸")
st_lottie(lottie_url, key="user")

st.title("ëª¨ë¹Œ, ë‹¹ì‹ ì˜ ëª¨ë“  ì´ë™ì„ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ğŸš– v2")
query = st.text_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?", value="")


# TODO(developer): Update and un-comment below lines
PROJECT_ID = "solen-demo-checkride-2"
display_name = "faq_checkride_2_rag"
rag_corpus_id = "4611686018427387904"
paths = ["gs://faq_checkride_2_html/about_mobil.html", "gs://faq_checkride_2_html/ê¸°íƒ€.html", "gs://faq_checkride_2_html/ë²•ì¸ë‹¨ì²´,ê·¸ë£¹.html", "gs://faq_checkride_2_html/ì˜ˆì•½,ê²°ì œ.html", "gs://faq_checkride_2_html/ì°¨ëŸ‰ì´ìš©,ì‚¬ê³ .html","gs://faq_checkride_2_html/íƒˆí‡´.html","gs://faq_checkride_2_html/í”„ë¡œëª¨ì…˜,ì¿ í°.html", "gs://faq_checkride_2_html/íšŒì›ê°€ì….html"] 
corpus_name = "projects/{PROJECT_ID}/locations/us-central1/ragCorpora/{rag_corpus_id}"

vertexai.init(project=PROJECT_ID, location="us-central1")

embedding_model_config = rag.RagEmbeddingModelConfig(
    vertex_prediction_endpoint=rag.VertexPredictionEndpoint(
        publisher_model="publishers/google/models/text-embedding-005"
    )
)

rag_corpus = rag.create_corpus(
    display_name=display_name,
    backend_config=rag.RagVectorDbConfig(
        rag_embedding_model_config=embedding_model_config
    ),
)
# Import Files to the RagCorpus
rag.import_files(
    rag_corpus.name,
    paths=paths,
    # Optional
    transformation_config=rag.TransformationConfig(
        chunking_config=rag.ChunkingConfig(
            chunk_size=512,
            chunk_overlap=100,
        ),
    ),
    max_embedding_requests_per_min=1000,  # Optional
)

# Direct context retrieval
rag_retrieval_config = rag.RagRetrievalConfig(
    top_k=1,  # Optional
    filter=rag.Filter(vector_distance_threshold=0.5),  # Optional
)
# response = rag.retrieval_query(
#     rag_resources=[
#         rag.RagResource(
#             rag_corpus=rag_corpus.name,
#             # Optional: supply IDs from `rag.list_files()`.
#             # rag_file_ids=["rag-file-1", "rag-file-2", ...],
#         )
#     ],
#     text=query,
#     rag_retrieval_config=rag_retrieval_config,
# )
# print(response.text)
# st.write(response.text)

# Enhance generation
# Create a RAG retrieval tool
rag_retrieval_tool = Tool.from_retrieval(
    retrieval=rag.Retrieval(
        source=rag.VertexRagStore(
            rag_resources=[
                rag.RagResource(
                    rag_corpus=rag_corpus.name,  # Currently only 1 corpus is allowed.
                    # Optional: supply IDs from `rag.list_files()`.
                    # rag_file_ids=["rag-file-1", "rag-file-2", ...],
                )
            ],
            rag_retrieval_config=rag_retrieval_config,
        ),
    )
)

# Create a Gemini model instance
rag_model = GenerativeModel(
    model_name="gemini-2.5-flash-preview-05-20", tools=[rag_retrieval_tool]
)

if query:
    with st.spinner("ëª¨ë¹ŒëŸ¬ê°€ ë‹µë³€ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤..."):
        st.write(f"'{query}'...ì²˜ë¦¬ì¤‘... ")
    response = rag_model.generate_content(query)
    st.write(response.text)


if query:
    # Generate response
        response = rag.retrieval_query(
            rag_resources=[
                rag.RagResource(
                    rag_corpus=rag_corpus.name,
                    # Optional: supply IDs from `rag.list_files()`.
                    # rag_file_ids=["rag-file-1", "rag-file-2", ...],
                )
            ],
            text=query,
            rag_retrieval_config=rag_retrieval_config,
        )
        if response.contexts:
            st.subheader("ì—°ê´€ëœ ì •ë³´ë¥¼ ì¶”ì²œë“œë ¤ìš” ğŸ™")
            # response=response[0] # ì´ ì¤„ì„ ì‚­ì œí•©ë‹ˆë‹¤.
            # print(response.contexts) # This prints the RagContexts wrapper object

            # Iterate over the actual list of context items,
            # which is likely an attribute of response.contexts
            actual_contexts_list = []
            if hasattr(response.contexts, 'contexts') and hasattr(response.contexts.contexts, '__iter__'):
                actual_contexts_list = response.contexts.contexts
            
            for context in actual_contexts_list: 
                if context.text:
                    st.write(context.text) # Use context.text
                if context.source_uri:
                    st.caption(f"Document: {context.source_uri}") # Use context.source_uri
                st.divider()
        else:
            st.write("ì €í¬ FAQì„œë¹„ìŠ¤ì— ì—†ëŠ” ë¬¸ì˜ì‚¬í•­ì…ë‹ˆë‹¤. ì£„ì†¡í•©ë‹ˆë‹¤.")
       
# Example response:
#   RAG stands for Retrieval-Augmented Generastion.
#   It's a technique used in AI to enhance the quality of responses
# ...
